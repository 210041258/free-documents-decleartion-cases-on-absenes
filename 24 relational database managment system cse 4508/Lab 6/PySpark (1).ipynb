{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to (Py)Spark\n",
        "\n",
        "* Tim Hopper – [@tdhopper](http://www.twitter.com/tdhopper) – Raleigh, NC\n",
        "* Developer and Anecdote Scientist\n",
        "* IPython Notebook available at http://tinyurl.com/PySpark\n",
        "\n",
        "<center>![](http://ak.picdn.net/shutterstock/videos/5081630/preview/stock-footage-electrical-spark-and-smoke-between-two-insulated-copper-wires-looped.jpg) </center>"
      ],
      "metadata": {
        "id": "7AmUvygUOr80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to (Py)Spark\n",
        "\n",
        "> Apache Spark™ is a fast and general engine for large-scale data processing."
      ],
      "metadata": {
        "id": "OCAq9o1lOr80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> It provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
        "\n",
        "[source](http://spark.apache.org/docs/latest/)"
      ],
      "metadata": {
        "id": "fGyXRQWVOr81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to (Py)Spark\n",
        "\n",
        "* Originally developed at Berkeley's AMPLab in 2009.\n",
        "* BSD-ed in 2010.\n",
        "* Donated to Apache in 2013.\n",
        "* Apache Top-Level Project in 2014.\n",
        "* 1.0.0 released in May 2014.\n",
        "* Currently on 1.2.0 (released December 2014).\n",
        "* Backed by Databricks (databricks.com)."
      ],
      "metadata": {
        "id": "dXJyYvcsOr81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example"
      ],
      "metadata": {
        "id": "UFnMHNmmOr81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sum the squares of the integers from 1 to 10."
      ],
      "metadata": {
        "id": "6bTYVGyYOr82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySparkExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Access the Spark Context from the Spark Session\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "GnKe1brFPT6F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).map(lambda x: x**2).sum()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "385"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "metadata": {
        "id": "thGLMPsxOr82",
        "outputId": "1c2f9486-78ca-42f2-a108-fedfc3644cea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example"
      ],
      "metadata": {
        "id": "rRg49DyGOr84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words(\"english\"))\n"
      ],
      "metadata": {
        "id": "CGJJrpLOPfwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e2b1f3-3d5c-41a8-ee5d-da725811e494"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import nltk.corpus as corpus\n",
        "    stopwords = set(corpus.stopwords.words())\n",
        "except ImportError:\n",
        "    stopwords = []"
      ],
      "outputs": [],
      "metadata": {
        "id": "gK3JI4SqOr84"
      },
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary module to upload files in Colab\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "AeolUmi2QXuq",
        "outputId": "db0533b5-debd-41a1-91a7-004fc25f4c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3fb1689b-e201-499d-836d-9cdce311a63a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3fb1689b-e201-499d-836d-9cdce311a63a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the stopwords list defined\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Define stopwords or use an existing list like nltk's\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Adjust the filename based on the uploaded file\n",
        "filename = next(iter(uploaded.keys()))\n",
        "\n",
        "# Now, use your RDD transformation pipeline with Python 3 compatible syntax\n",
        "rdd = sc.textFile(filename)\n",
        "common_words = (rdd\n",
        "                .flatMap(lambda line: re.findall(r'\\w+', line.lower()))  # Split by words and lowercase\n",
        "                .filter(lambda word: word not in stopwords)              # Remove stopwords\n",
        "                .map(lambda word: (word, 1))                             # Map each word to (word, 1)\n",
        "                .reduceByKey(lambda a, b: a + b)                         # Sum occurrences\n",
        "                .map(lambda x: (x[1], x[0]))                             # Swap to (count, word) for sorting\n",
        "                .top(10))                                                # Get top 10 by count\n",
        "\n",
        "# Display the result\n",
        "print(\"Most common words:\", common_words)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "1sNEg6PtOr84"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "\n",
        "These examples is running locally on my laptop.\n",
        "\n",
        "The data file (example.txt) is loaded into a _local_ Resilient Distributed Dataset (__RDD__).\n",
        "\n",
        "If my Spark Context (`sc`) were created on a Spark cluster, the data would have be _partitioned_ across the worker nodes."
      ],
      "metadata": {
        "id": "r_oIXSGPOr84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "\n",
        "(Py)spark evaluates expressions lazily. \"The transformations are only computed when an action requires a result to be returned to the driver program.\" [source](http://spark.apache.org/docs/1.2.0/programming-guide.html#rdd-operations)"
      ],
      "metadata": {
        "id": "hxvaqPAbOr85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd = sc.parallelize(range(10**8)).map(lambda x: float(x) ** 2)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "tInpsHiqOr85"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "_ = rdd.count()"
      ],
      "outputs": [],
      "metadata": {
        "id": "cB-Ld5yROr85"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark vs Pyspark?\n",
        "\n",
        "Spark is written in Scala. The 'native' API is in Scala.\n",
        "\n",
        "Pyspark is a very lightweight wrapper around the native API. (You can see its implementation [here](https://github.com/apache/spark/tree/master/python/pyspark).)"
      ],
      "metadata": {
        "id": "9brhfmZjOr85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](http://i.imgur.com/YlI8AqEl.png)\n",
        "\n",
        "[source](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)"
      ],
      "metadata": {
        "id": "JtruE9ccOr85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark vs Pyspark?\n",
        "\n",
        "__Key difference:__\n",
        "\n",
        "* Python (unlike Scala) is dynamically typed. (RDDs can hold objects of multiple types!)\n",
        "* Pyspark sometimes lags behind Spark in feature releases.\n",
        "\n",
        "(There's also a Java API in case you really hate life.)"
      ],
      "metadata": {
        "id": "wzjv7G7XOr85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark vs Pyspark?\n",
        "\n",
        "It must be slower, right?"
      ],
      "metadata": {
        "id": "JyJ0ARjBOr85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Spark’s core developers have worked extensively to bridge the performance gap between JVM languages and Python.\n",
        "\n",
        "> In particular, PySpark can now run on PyPy to leverage the just-in-time compiler. (Up to 50x speedup)\n",
        "\n",
        "> The way Python processes communicate with the main Spark JVM programs have also been redesigned to enable worker reuse.\n",
        "\n",
        "[source](http://radar.oreilly.com/2015/02/recent-performance-improvements-in-apache-spark.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "esQ_UyzKOr85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is this better than Hadoop?"
      ],
      "metadata": {
        "id": "U7uRAJDrOr85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Major difference:__\n",
        "\n",
        "Spark keep data in worker memory while tends to keep data on disk.\n",
        "\n",
        "According to the Spark webpage it can run \"100x faster than Hadoop by exploiting in memory computing and other optimizations \"\n"
      ],
      "metadata": {
        "id": "gOllubx6Or85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is this better than Hadoop?"
      ],
      "metadata": {
        "id": "f0Yx7TdROr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Spark officially sets a new record in large-scale sorting\n",
        "\n",
        "> Using Spark on 206 EC2 machines, we sorted 100 TB of data on disk in 23 minutes. In comparison, the previous world record set by Hadoop MapReduce used 2100 machines and took 72 minutes. This means that Spark sorted the same data 3X faster using 10X fewer machines.\n",
        "\n",
        "[source](http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html)"
      ],
      "metadata": {
        "id": "pH4R3hQmOr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is this better than Hadoop?\n",
        "\n",
        "__Also:__\n",
        "\n",
        "__RDD__ is a key development: RDD's provide \"immutable resilient  distributed collection of records\"."
      ],
      "metadata": {
        "id": "UvIKmpc1Or86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Unlike existing storage\n",
        "abstractions for clusters, which require data replication\n",
        "for fault tolerance, RDDs offer an API based on coarsegrained\n",
        "transformations that lets them recover data efficiently using lineage.\n",
        "\n",
        "See: [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for\n",
        "In-Memory Cluster Computing](http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf), [Spark: Cluster Computing with Working Sets](https://amplab.cs.berkeley.edu/wp-content/uploads/2011/06/Spark-Cluster-Computing-with-Working-Sets.pdf), [Spark Research](https://spark.apache.org/research.html)"
      ],
      "metadata": {
        "id": "0TNAwHniOr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is this better than Hadoop?\n",
        "\n",
        "__Also:__\n",
        "    \n",
        "Spark provides 80+ high(er)-level, functional-style operators beyond simple \"map\" and \"reduce\". (Not even to mention high-level tools Spark Streaming, Spark SQL, MLib, and GraphX.)\n",
        "\n",
        "For example:\n",
        "\n",
        "* count\n",
        "* countApprox\n",
        "* flatMap\n",
        "* filter\n",
        "* flatMap\n",
        "* groupBy\n",
        "* map\n",
        "* reduce\n",
        "* reduceByKey\n",
        "* sample\n",
        "* sortBy\n",
        "* union"
      ],
      "metadata": {
        "id": "D0sPy6G3Or86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is this better than Hadoop?\n",
        "\n",
        "__Native Python Code:__\n",
        "\n",
        "* Unlike Hive/Pig\n",
        "\n",
        "__No Java:__\n",
        "\n",
        "* Unlike native Hadoop\n",
        "\n",
        "__High(er)-level operators:__\n",
        "\n",
        "* Unlike mrjob\n",
        "\n",
        "__Functional style:__\n",
        "\n",
        "> Spark imitates Scala’s collections API and functional style, which is a boon to Java and Scala developers, but also somewhat familiar to developers coming from Python. [source](http://blog.cloudera.com/blog/2014/03/why-apache-spark-is-a-crossover-hit-for-data-scientists/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IgSLLtu_Or86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[pyspark-pictures](http://nbviewer.ipython.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb) is a handy help for the Spark API:\n",
        "\n",
        "```\n",
        "rdd1.cartesian(rdd2)\n",
        "```\n",
        "\n",
        "![](http://nbviewer.ipython.org/github/jkthompson/pyspark-pictures/blob/master/images/pyspark-page17.svg)"
      ],
      "metadata": {
        "id": "oSmFOgNvOr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing (Py)Spark locally\n",
        "\n",
        "For Mac users using [Homebrew](http://brew.sh/):\n",
        "\n",
        "```\n",
        "$ brew install apache-spark\n",
        "```\n",
        "\n",
        "Install [Java SDK](http://www.oracle.com/technetwork/java/javase/downloads/index.html)"
      ],
      "metadata": {
        "id": "6R1mcxInOr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launching the Pyspark REPL\n",
        "\n",
        "\n",
        "```\n",
        "$ IPYTHON=1 pyspark\n",
        "```"
      ],
      "metadata": {
        "id": "3s441gvNOr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see:\n",
        "\n",
        "```\n",
        "Welcome to\n",
        "      ____              __\n",
        "     / __/__  ___ _____/ /__\n",
        "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
        "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.2.1\n",
        "      /_/\n",
        "\n",
        "Using Python version 2.7.6 (default, Sep  9 2014 15:04:36)\n",
        "SparkContext available as sc.\n",
        ">>>\n",
        "```"
      ],
      "metadata": {
        "id": "77sx0J8zOr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launching the Pyspark in an IPython notebook\n",
        "\n",
        "```\n",
        "$ IPYTHON_OPTS=\"notebook --matplotlib inline\" pyspark\n",
        "```"
      ],
      "metadata": {
        "id": "S0XWznrXOr86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a special IPython notebook that is initialized with a SparkContext object called `sc`:"
      ],
      "metadata": {
        "id": "yBK3LTOROr86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "outputs": [],
      "metadata": {
        "id": "GK156ad3Or86"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(You can also create [IPython profiles](http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/) [to automate some of this](http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/).)"
      ],
      "metadata": {
        "id": "xHLcN2zpOr87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These commands will start Pyspark in __local__ mode. As opposed to __cluster__ mode."
      ],
      "metadata": {
        "id": "nsUrVSelOr87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exact same code can be run in local and cluster modes! It just depends on how you initialize your Spark session."
      ],
      "metadata": {
        "id": "EaQX3YGKOr87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Data"
      ],
      "metadata": {
        "id": "yLKxfjYSOr87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Python iterable into an RDD\n",
        "\n",
        "sc.parallelize(range(10))"
      ],
      "outputs": [],
      "metadata": {
        "id": "oINJi5QuOr9B"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a text file\n",
        "\n",
        "sc.textFile(\"example.txt\") # Each line is a separate element in the RDD"
      ],
      "outputs": [],
      "metadata": {
        "id": "o7Z3wvuuOr9B"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text files\n",
        "\n",
        "sc.textFile(\"example.txt,example2.txt\").collect()[-1001:-991]"
      ],
      "outputs": [],
      "metadata": {
        "id": "7HXFuvYWOr9B"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "These can be used to load text files from Amazon S3."
      ],
      "metadata": {
        "id": "A8_n3XVQOr9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SparkContext.wholeTextFile`...\n",
        "\n",
        "> ...lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with <code>textFile</code>, which would return one record per line in each file\n"
      ],
      "metadata": {
        "id": "tpxrNTW9Or9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SparkContext.newAPIHadoopRDD`\n",
        "\n",
        "> PySpark can also read any Hadoop InputFormat or write any Hadoop OutputFormat, for both ‘new’ and ‘old’ Hadoop MapReduce APIs.\n",
        "\n",
        "For example, [Cassandra](https://github.com/apache/spark/blob/master/examples/src/main/python/cassandra_inputformat.py)."
      ],
      "metadata": {
        "id": "aXCG4YSUOr9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving Data"
      ],
      "metadata": {
        "id": "Kjg9O3PNOr9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`rdd.collect()` converts a RDD object into a Python list on the host machine.\n",
        "\n",
        "`rdd.saveAsTextFile()` saves an RDD as a string. See also `rdd.saveAsPickleFile()`.\n",
        "\n",
        "`rdd.saveAsNewAPIHadoopDataset()` saves an RDD object to a Hadoop data source (e.g. HDFS, [Cassandra](https://github.com/Parsely/pyspark-cassandra))."
      ],
      "metadata": {
        "id": "y3a6ygtEOr9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manipulating Data"
      ],
      "metadata": {
        "id": "5E46VVPMOr9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort last three presidents by last name"
      ],
      "metadata": {
        "id": "_ngj4eILOr9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([\"Barack Hussein Obama\", \"George Walker Bush\", \"William Jefferson Clinton\"])\n",
        "\n",
        "rdd.sortBy(keyfunc=lambda k: k.split(\" \")[-1]).collect()"
      ],
      "outputs": [],
      "metadata": {
        "id": "v0rkhmIqOr9C"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manipulating Data"
      ],
      "metadata": {
        "id": "-8KhNAuAOr9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join Datasets"
      ],
      "metadata": {
        "id": "Q_r6S4PpOr9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3)])\n",
        "rdd2 = sc.parallelize([(\"a\", 6), (\"b\", 7), (\"b\", 8), (\"d\", 9)])"
      ],
      "outputs": [],
      "metadata": {
        "id": "f7scq1ijOr9C"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.join(rdd2).collect()"
      ],
      "outputs": [],
      "metadata": {
        "id": "xPTVuMbmOr9C"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.fullOuterJoin(rdd2).collect()"
      ],
      "outputs": [],
      "metadata": {
        "id": "YC6LkcMBOr9C"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manipulating Data"
      ],
      "metadata": {
        "id": "9oPI7a4OOr9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext, Row\n",
        "from pyspark.sql.types import StructField, StructType, FloatType\n",
        "import pandas as pd\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "NAhMKTD8Or9C"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load in the [Wake County Real Estate Data](http://www.wakegov.com/tax/realestate/redatafile/pages/default.aspx)."
      ],
      "metadata": {
        "id": "5_cXoykpOr9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_real_estate = sc.textFile(\"all.txt\")\n",
        "print(raw_real_estate.take(1)[0][:250])\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "kHkVdK4lOr9D"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"http://i.imgur.com/EiPTMML.gif\" title=\"source: imgur.com\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "K94vCOoYOr9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manipulating Data"
      ],
      "metadata": {
        "id": "BreaPdhXOr9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize SparkSession (replaces SQLContext)\n",
        "spark = SparkSession.builder.appName(\"RealEstateApp\").getOrCreate()\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "omCSwKw2Or9D"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if data is loaded correctly\n",
        "print(\"Initial data load sample:\")\n",
        "print(raw_real_estate.take(5))  # Print a sample to verify loading\n",
        "\n",
        "# Map raw data into a dictionary format for each row\n",
        "wake_county_real_estate = raw_real_estate.map(lambda row:\n",
        "    dict(\n",
        "     owner = row[0:35].strip().title(),\n",
        "     last_name = row[0:35].strip().title().split(\",\")[0],\n",
        "     address = row[70:105].strip().title(),\n",
        "     sale_price = int(row[273:284].strip() or -1),\n",
        "     value = int(row[305:316].strip() or -1),\n",
        "     use = int(row[653:656].strip() or -1),\n",
        "     heated_area = int(row[471:482].strip() or -1),\n",
        "     year_built = int(row[455:459].strip() or -1),\n",
        "     height = row[509:510].strip(),\n",
        "    ))\n",
        "\n",
        "# Check the mapped data to verify field extraction\n",
        "print(\"Mapped data sample:\")\n",
        "print(wake_county_real_estate.take(5))\n",
        "\n",
        "# Convert the mapped data to an RDD of Rows\n",
        "wake_county_real_estate_rdd = wake_county_real_estate.map(lambda d: Row(**d))\n",
        "\n",
        "# Create a DataFrame from the RDD\n",
        "wake_df = spark.createDataFrame(wake_county_real_estate_rdd)\n",
        "\n",
        "# Register the DataFrame as a SQL temporary view\n",
        "wake_df.createOrReplaceTempView(\"wake\")\n",
        "\n",
        "# Step 1: Check if data exists in the DataFrame by querying all rows without filters\n",
        "print(\"Sample data from 'wake' table (no filters):\")\n",
        "wake_df.show(10)\n",
        "\n",
        "# Step 2: Apply each filter step-by-step to identify which might be causing issues\n",
        "\n",
        "# First, filter by `use = 66` (for church buildings)\n",
        "print(\"Sample data where use = 66:\")\n",
        "result_df_use = spark.sql(\"SELECT * FROM wake WHERE use = 66 LIMIT 10\")\n",
        "result_df_use.show()\n",
        "\n",
        "# Then add `value > 4000000`\n",
        "print(\"Sample data where use = 66 and value > 4000000:\")\n",
        "result_df_value = spark.sql(\"SELECT * FROM wake WHERE use = 66 AND value > 4000000 LIMIT 10\")\n",
        "result_df_value.show()\n",
        "\n",
        "# Finally, add the filter `owner LIKE '%Church%'`\n",
        "print(\"Final query with all filters:\")\n",
        "result_df_final = spark.sql(\"\"\"\n",
        "    SELECT DISTINCT owner, address, year_built, value\n",
        "    FROM wake\n",
        "    WHERE value > 4000000 AND\n",
        "          use = 66 AND\n",
        "          owner LIKE '%Church%'\n",
        "\"\"\")\n",
        "result_df_final.show()\n",
        "\n",
        "# If needed, convert to pandas for further analysis\n",
        "pd_df = result_df_final.toPandas()\n",
        "print(pd_df)"
      ],
      "metadata": {
        "id": "fGrXx_kQT5kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manipulating Data"
      ],
      "metadata": {
        "id": "hw7vquMqOr9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Who owns the most expensive church buildings in Raleigh?"
      ],
      "metadata": {
        "id": "gPgmv_1cOr9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the unique values in the 'use' field to confirm expected values\n",
        "spark.sql(\"SELECT DISTINCT use FROM wake\").show()\n",
        "\n",
        "# Check sample values in the 'value' field to confirm it contains numeric data\n",
        "spark.sql(\"SELECT value FROM wake ORDER BY value DESC LIMIT 10\").show()\n",
        "\n",
        "# Check owners to ensure they contain the keyword 'Church' where expected\n",
        "spark.sql(\"SELECT owner FROM wake WHERE owner LIKE '%Church%' LIMIT 10\").show()\n"
      ],
      "metadata": {
        "id": "cJCgoJoJUE5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manipulating Data"
      ],
      "metadata": {
        "id": "j3K86CCcOr9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the 43rd richest American's house worth?"
      ],
      "metadata": {
        "id": "GkazjNfOOr9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the SQL query to find the maximum value for the specified owner\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT MAX(value) AS price\n",
        "    FROM wake\n",
        "    WHERE owner LIKE 'Goodnight, James H% & Ann B%'\n",
        "    GROUP BY last_name\n",
        "\"\"\")\n",
        "\n",
        "# Collect the result and check if it's non-empty\n",
        "price_data = result.collect()\n",
        "\n",
        "# Safely access the price if a result exists\n",
        "if price_data:\n",
        "    max_price = price_data[0]['price']\n",
        "    print(\"Max Price:\", max_price)\n",
        "else:\n",
        "    print(\"No results found for the specified owner.\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "1RWTvg2mOr9D"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(We could have done these same queries with the 'native' Spark functional method chaining.)"
      ],
      "metadata": {
        "id": "ZSaT8EUxOr9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the SQL query to get the maximum value for the specified church\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT MAX(value) AS price\n",
        "    FROM wake\n",
        "    WHERE owner = 'Crosspointe Church At Cary'\n",
        "\"\"\")\n",
        "\n",
        "# Collect the result and check if it's non-empty\n",
        "price_data = result.collect()\n",
        "\n",
        "# Access the price if a result exists\n",
        "if price_data:\n",
        "    max_price = price_data[0]['price']\n",
        "    print(\"Max Price for Crosspointe Church At Cary:\", max_price)\n",
        "else:\n",
        "    print(\"No results found for the specified church.\")\n"
      ],
      "metadata": {
        "id": "GU3hk0cPV9i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manipulating Data"
      ],
      "metadata": {
        "id": "w2-Iala1Or9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, if you wanted to load terabytes of real estate data from HDFS or S3 (for example), you could run this exact same code on a Spark cluster."
      ],
      "metadata": {
        "id": "omYgUKcYOr9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"http://41.media.tumblr.com/7b2b5be9520a1f7a5deebc49a78bc5ce/tumblr_n6ck5rrrBy1td9006o1_1280.jpg\" width=500>"
      ],
      "metadata": {
        "id": "ZBrCWYx5Or9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Frames! (Coming Soon in 1.3)"
      ],
      "metadata": {
        "id": "BgeTKfRLOr9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructs a DataFrame from the users table in Hive.\n",
        "\n",
        "```python\n",
        "users = sc.table(\"users\")\n",
        "```\n",
        "\n",
        "Create a new DataFrame that contains “young users” only\n",
        "\n",
        "```python\n",
        "young = users.filter(users.age < 21)\n",
        "```\n",
        "    \n",
        "Count the number of young users by gender\n",
        "    \n",
        "```python\n",
        "young.groupBy(\"gender\").count()\n",
        "```"
      ],
      "metadata": {
        "id": "YLrcpz5eOr9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Frames! (Coming Soon in 1.3)"
      ],
      "metadata": {
        "id": "n8_Uxm1BOr9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From JSON files in S3\n",
        "\n",
        "```python\n",
        "logs = sc.load(\"s3n://path/to/data.json\", \"json\")\n",
        "```\n",
        "\n",
        "Join young users with another DataFrame called logs\n",
        "    \n",
        "```python\n",
        "young.join(logs, logs.userId == users.userId, \"left_outer\")\n",
        "```\n",
        "\n",
        "\n",
        "[source](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)"
      ],
      "metadata": {
        "id": "xGZoccabOr9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Frames! (Coming Soon in 1.3)"
      ],
      "metadata": {
        "id": "28e77Nz6Or9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"http://blogs.edweek.org/edweek/the_startup_blog/assets_c/2014/02/a78ae76da19c1a0f9e0e9b2f7e6229e70bd36cf7bc5b2f29b5f8900face50234%5B1%5D-thumb-autox384-6965.jpg\" width=\"200\"></center>\n",
        "\n",
        "Convert Spark DataFrame to Pandas\n",
        "\n",
        "```\n",
        "pandas_df = young.toPandas()\n",
        "```\n",
        "\n",
        "Create a Spark DataFrame from Pandas\n",
        "\n",
        "```\n",
        "spark_df = context.createDataFrame(pandas_df)\n",
        "```"
      ],
      "metadata": {
        "id": "EtwS7z_uOr9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning with (Py)Spark"
      ],
      "metadata": {
        "id": "ISgPAfjzOr9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.tree import DecisionTree, LabeledPoint\n",
        "from pyspark.mllib import feature\n",
        "from pyspark.mllib.stat import Statistics\n",
        "from random import choice"
      ],
      "outputs": [],
      "metadata": {
        "id": "oU_G7WfrOr9E"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subset to Apartment Buildings and Office Buildings"
      ],
      "metadata": {
        "id": "GwDc7cUzOr9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset = wake_county_real_estate.filter(lambda d:\n",
        "                                        d[\"use\"] in [7, 34])\n",
        "subset = subset.filter(lambda d: d[\"heated_area\"] > 0\n",
        "              and d[\"year_built\"] > 1900) \\\n",
        "              .map(lambda d: LabeledPoint(\n",
        "                                1 if d[\"use\"] == 7 else 0,\n",
        "                                [d[\"year_built\"],\n",
        "                                 d[\"heated_area\"]]))\n",
        "\n",
        "subset.take(2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "a8tlETI-Or9E"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ABC, 123123123\")"
      ],
      "metadata": {
        "id": "gO8yNjnKYUxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ewKn3x5km79W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " print(\"AHMED M. S. ALBREEM, 210041258\")\n",
        ""
      ],
      "metadata": {
        "id": "pXnj3xIjYVKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}